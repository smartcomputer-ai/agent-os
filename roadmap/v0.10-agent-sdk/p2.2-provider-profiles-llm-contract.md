# P2.2: Provider Profiles and LLM Contract Evolution

**Stage**: P2.2  
**Status**: Proposed  
**Depends on**: P2.1

## Implementation Progress (2026-02-13)

Completed scaffold work:

1. Added P2.2 agent contract scaffolding in SDK AIR assets:
   - `aos.agent/ProviderProfile@1`
   - `aos.agent/LlmToolCall@1`
   - `aos.agent/LlmToolCallList@1`
   - profile lookup contracts:
     - `aos.agent/ProfileLookupRequested@1`
     - `aos.agent/ProfileLookupResolved@1`
     - `aos.agent/ProfileLookupFailure@1`
     - `aos.agent/ProfileLookupFailed@1`
     - `aos.agent/ProfileCatalogState@1`
     - `aos.agent/ProfileCatalogEvent@1`
2. Added Rust contract scaffolding:
   - `crates/aos-agent-sdk/src/contracts/profile.rs`
   - `crates/aos-agent-sdk/src/contracts/llm.rs`
   - wired into `crates/aos-agent-sdk/src/contracts/mod.rs`
3. Added profile catalog reducer module scaffold:
   - `crates/aos-agent-sdk/src/bin/profile_catalog_reducer.rs`
   - module def in `crates/aos-agent-sdk/air/module.air.json`
   - manifest wiring in `crates/aos-agent-sdk/air/manifest.air.json`
4. Build status:
   - `cargo check -p aos-agent-sdk` passes.

Remaining for full P2.2 completion:

1. Rebaseline contract ownership after boundary issue:
   - remove SDK-local `sys/Llm*` schema ownership from `crates/aos-agent-sdk/air/*`,
   - keep SDK AIR assets scoped to `aos.agent/*`.
2. Implement core-side `sys/Llm*` contract evolution in:
   - `spec/defs`,
   - `aos-effects`,
   - `aos-host`,
   - `aos-sys`/kernel cap enforcers and tests.
3. Implement deterministic mapper flow from SDK run/step state to core `sys/LlmGenerateParams@1`.
4. Implement profile lookup request/resolution/failure runtime flow between session and catalog reducers.
5. Add deterministic integration coverage (OpenAI/Anthropic profile paths, unknown profile failure path, replay parity checks).

> Note: The previous scaffold that placed `sys/Llm*` shapes under SDK AIR assets is treated as a temporary draft artifact and not the authoritative architecture direction.

## Ownership Boundary (Authoritative)

1. Core owns `sys/*`:
   - built-in schemas/effects/caps under `spec/defs/*`,
   - Rust low-level effect/receipt types under `aos-effects`,
   - adapter execution under `aos-host`,
   - cap enforcer decoding/constraints under `aos-sys` and kernel fallbacks.
2. SDK owns `aos.agent/*`:
   - profile/session/run/turn/step contracts,
   - profile lookup choreography,
   - mapper helpers producing core effect params,
   - reducer-facing typed envelopes/events.
3. Rule:
   - SDK AIR assets must not define or replace `sys/*` schemas.
   - `sys/*` schemas must not depend on `aos.agent/*` references.

## Purpose

Standardize provider strategy and align LLM effect contracts with agent runtime controls.

This stage resolves concerns #2 (provider philosophy) and #7 (LLM contract pressure).

Because v0.10 accepts breaking changes, this stage replaces `sys/LlmGenerateParams@1` and `sys/LlmGenerateReceipt@1` in place instead of introducing parallel `@2` schemas.

This document is the normative source of truth for P2.2 provider/LLM contracts and boundary ownership.

## Why Provider Profiles Exist

`provider + model` is not enough for reliable behavior. Different providers differ on:

- tool-call protocol and shape,
- reasoning controls,
- streaming semantics,
- context-window characteristics,
- option passthrough format.

`ProviderProfile` is the contract that captures those differences once so app reducers/plans do not fork behavior per provider.

## Catalog-Backed, Thin Profiles

`crates/aos-llm/src/catalog_models.json` already carries model metadata (context window, capabilities, aliases).

P2.2 should not duplicate that whole catalog in AIR schemas.

Design split:

1. Catalog (`catalog_models.json`):
   - runtime metadata and compatibility hints.
2. AIR profile schema:
   - world-pinned selection and defaults/run overrides needed for deterministic behavior and audit.

Key rule:

Anything that affects cap/policy decisions or effect intent semantics must be explicit in `LlmGenerateParams` (and thus journaled), not hidden behind mutable host catalog lookups.

Determinism boundary:

1. Catalog lookup may validate compatibility and produce diagnostics only.
2. Catalog lookup may not inject or mutate cap/policy-relevant values.
3. If required resolved values are missing after applying run-scoped config and profile defaults, run start must fail with typed validation error rather than infer host-side defaults.

## What This Stage Locks

1. `aos.agent/ProviderProfile@1` schema and semantics.
2. Core-owned in-place replacement/evolution of `sys/LlmGenerateParams@1` and `sys/LlmGenerateReceipt@1`.
3. Normalized output envelope contract for `output_ref`.
4. Host adapter wiring from typed effect params into `aos-llm::Request`.
5. Profile lookup architecture using AOS primitives only:
   - dedicated `aos.agent/ProfileCatalogReducer@1`,
   - event-driven lookup between session run request and run start,
   - no standalone host-side "resolver service".

## Scope

1. Define provider profile contract:
   - `aos.agent/ProviderProfile@1`
   - thin world-pinned defaults (`provider`, `model`, default refs)
   - optional policy-level constraints (`allowed_models`, local max token cap)
   - catalog linkage via model id hints
2. Evolve LLM effect schemas in core (not SDK):
   - replace `sys/LlmGenerateParams@1` in place
   - replace `sys/LlmGenerateReceipt@1` in place
3. Thread profile/runtime controls through host adapter:
   - nested runtime args (`runtime`)
   - reasoning/tool/stop/metadata/options controls
4. Keep normalized + raw output model:
   - `output_ref` for reducer-facing normalized envelope
   - `raw_output_ref` for provider-native audit/debug
5. Define profile lookup/reconciliation flow with AOS primitives:
   - session reducer emits lookup request event with selected `provider_profile_id`,
   - profile catalog reducer returns resolved profile event (or typed failure),
   - session reducer snapshots immutable `active_run_config` only after successful lookup.

## Why Nested `runtime` in `LlmGenerateParams@1`

With profile-first run-scoped configuration, nesting keeps contracts clearer:

- top-level in `LlmGenerateParams@1`:
  - identity and determinism-critical routing/audit fields (`correlation_id`, `provider_profile_id`, `provider`, `model`, `message_refs`, `api_key`)
- nested `runtime`:
  - execution payload that combines:
    - run-pinned optional controls (`max_tokens`, `reasoning_effort`) copied from immutable `active_run_config`
    - step-local fields (`temperature`, tool/stop/options/format refs) derived deterministically by reducer/plan logic

This keeps run-scoped configuration separate from step-scoped execution data.
P2.2 does not define host/user step-level configuration overrides.
Any step-local variation must come from deterministic reducer/plan logic and current step context.

`provider` and `model` are intentionally not in `runtime` so they cannot drift mid-run as per P2.1 run-immutability rules.

## Proposed Schema Shapes (AIR-style)

### Provider Profile Identity

`aos.agent/ProviderProfileId@1` and `aos.agent/ReasoningEffort@1` are defined in P2.1 and reused here.

```json
{
  "$kind": "defschema",
  "name": "aos.agent/ProviderProfile@1",
  "type": {
    "record": {
      "profile_id": { "ref": "aos.agent/ProviderProfileId@1" },
      "provider": { "text": {} },
      "default_model": { "text": {} },
      "catalog_model_id": { "option": { "text": {} } },
      "allowed_models": { "option": { "set": { "text": {} } } },
      "max_tokens_cap": { "option": { "nat": {} } },
      "default_reasoning_effort": {
        "option": { "ref": "aos.agent/ReasoningEffort@1" }
      },
      "provider_options_default_ref": { "option": { "hash": {} } },
      "response_format_default_ref": { "option": { "hash": {} } }
    }
  }
}
```

This profile is intentionally thin.

- Provider capability flags (streaming/tool parallel/etc.) come from catalog/runtime metadata logic, not duplicated in AIR.
- `catalog_model_id` is optional linkage for host-side validation and UI hints.
- `provider`, `default_model`, and caps-relevant constraints remain explicit in AIR.

### LLM Runtime Args (core helper schema)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmRuntimeArgs@1",
  "type": {
    "record": {
      "temperature": { "option": { "dec128": {} } },
      "max_tokens": { "option": { "nat": {} } },
      "tool_refs": { "option": { "list": { "hash": {} } } },
      "tool_choice": { "option": { "ref": "sys/LlmToolChoice@1" } },
      "reasoning_effort": { "option": { "text": {} } },
      "stop_sequences": { "option": { "list": { "text": {} } } },
      "metadata": {
        "option": {
          "map": {
            "key": { "text": {} },
            "value": { "text": {} }
          }
        }
      },
      "provider_options_ref": { "option": { "hash": {} } },
      "response_format_ref": { "option": { "hash": {} } }
    }
  }
}
```

### LLM Params (replaced `@1`, now with nested runtime args)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateParams@1",
  "type": {
    "record": {
      "correlation_id": { "option": { "text": {} } },
      "provider_profile_id": { "option": { "text": {} } },
      "provider": { "text": {} },
      "model": { "text": {} },
      "message_refs": { "list": { "hash": {} } },
      "runtime": { "ref": "sys/LlmRuntimeArgs@1" },
      "api_key": { "option": { "ref": "sys/TextOrSecretRef@1" } }
    }
  }
}
```

Notes:

- Keep `provider` and `model` explicit in params for cap enforcement, policy evaluation, and audit.
- `provider_profile_id` is an optional audit/correlation field in core `sys/*` and is not interpreted as an SDK-specific type.
- `correlation_id` is an optional agent-agnostic tracing field for out-of-band telemetry correlation.
- `runtime` groups execution fields cleanly and avoids endless top-level field growth.
- `provider`, `model`, and `provider_profile_id` are sourced from immutable active run config.
- `reasoning_effort` in `sys/*` must remain agent-agnostic (for example text enum string), with SDK-level typing mapped before effect emission.
- Optional tuning controls (`runtime.reasoning_effort`, `runtime.max_tokens`) are sourced from active run config when set; when unset, adapter leaves them unset so provider defaults apply.

### LLM Receipt (replaced `@1`)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateReceipt@1",
  "type": {
    "record": {
      "output_ref": { "hash": {} },
      "raw_output_ref": { "option": { "hash": {} } },
      "provider_response_id": { "option": { "text": {} } },
      "finish_reason": {
        "record": {
          "reason": { "text": {} },
          "raw": { "option": { "text": {} } }
        }
      },
      "token_usage": {
        "record": {
          "prompt": { "nat": {} },
          "completion": { "nat": {} }
        }
      },
      "usage_details": {
        "option": {
          "record": {
            "reasoning_tokens": { "option": { "nat": {} } },
            "cache_read_tokens": { "option": { "nat": {} } },
            "cache_write_tokens": { "option": { "nat": {} } }
          }
        }
      },
      "warnings_ref": { "option": { "hash": {} } },
      "cost_cents": { "option": { "nat": {} } },
      "provider_id": { "text": {} }
    }
  }
}
```

## Normalized Output Envelope (`output_ref`)

`output_ref` contract ownership is intentionally split:

1. Core `sys/LlmGenerateReceipt@1.output_ref` remains an adapter-produced hash ref.
2. SDK-managed flows should treat that blob as `sys/LlmOutputEnvelope@1`.
3. Non-SDK worlds may still consume the same core envelope without any `aos.agent/*` dependency.

`output_ref` must point to a provider-agnostic typed envelope in CAS, sufficient for reducers to parse tool calls without provider-specific branching.

Canonical tool-call schema:

```json
{
  "$kind": "defschema",
  "name": "sys/LlmToolCall@1",
  "type": {
    "record": {
      "call_id": { "text": {} },
      "tool_name": { "text": {} },
      "arguments_ref": { "hash": {} },
      "provider_call_id": { "option": { "text": {} } }
    }
  }
}
```

```json
{
  "$kind": "defschema",
  "name": "sys/LlmToolCallList@1",
  "type": { "list": { "ref": "sys/LlmToolCall@1" } }
}
```

`arguments_ref` must reference canonical JSON object payload in CAS. Reducers must never parse provider-specific argument strings.

Envelope schema:

```json
{
  "$kind": "defschema",
  "name": "sys/LlmOutputEnvelope@1",
  "type": {
    "record": {
      "assistant_text": { "option": { "text": {} } },
      "tool_calls_ref": { "option": { "hash": {} } },
      "reasoning_ref": { "option": { "hash": {} } }
    }
  }
}
```

When `tool_calls_ref` is present, the referenced CAS payload must validate as
`sys/LlmToolCallList@1`.

Parallel tool intent normalization:

1. Adapter must preserve model-emitted call ids as `call_id`.
2. Adapter must canonicalize call ordering by emitted sequence and include that sequence in serialized list order.
3. Reducer must use `call_id` identity for idempotency and stale-result handling.

`raw_output_ref` preserves full provider-native output for debug/audit.

## Mapping to `aos-llm` and OpenAI Responses API

Current status in codebase:

- `aos-llm::Request` already supports:
  - `reasoning_effort`,
  - `provider_options`,
  - `response_format`,
  - `metadata`,
  - `stop_sequences`.
- `aos-host` LLM adapter currently populates:
  - `output_ref`,
  - `raw_output_ref`,
  - token usage,
  but drops some advanced fields from effect params.

Expected mapping example (OpenAI Responses):

1. `sys/LlmGenerateParams@1.model` -> `request.model`
2. `runtime.reasoning_effort` -> `request.reasoning_effort`
3. `runtime.provider_options_ref` JSON -> `request.provider_options.openai.*`
4. provider returns `response.id` -> `provider_response_id`
5. normalized envelope -> `output_ref`
6. raw provider JSON -> `raw_output_ref`

Equivalent flow for Anthropic maps to `provider_options.anthropic.*` and Messages-specific semantics under same receipt contract.

## AOS Primitive Architecture (No Standalone Resolver)

P2.2 adopts an event-driven architecture using reducers/plans/effects only:

1. `aos.agent/SessionReducer@1` remains keyed by `session_id` and owns `session_config` and `active_run_config`.
2. `aos.agent/ProfileCatalogReducer@1` is introduced as a dedicated world-level catalog authority for `ProviderProfile@1` entries.
3. Profile lookup is done by typed DomainEvents between these reducers, not by direct host callbacks.
4. Plans remain orchestration-only and materialize `llm.generate` params from immutable `active_run_config`.
5. Host adapter consumes fully resolved params and must not inject semantic defaults.

Seed/update model:

1. Profiles are seeded via SDK AIR assets and/or explicit profile-catalog events.
2. Optional workspace/catalog inspection can exist in plans, but authoritative profile values used for run config must flow through `ProfileCatalogReducer` and typed events.
3. Replay must reproduce identical profile lookup outcomes from journaled events and reducer state.

## Profile Lookup and Run-Start Resolution Semantics

1. At run start, select run request config as:
   - `RunRequested.run_overrides` when present (full replacement), otherwise
   - `session_config` defaults (P2.1).
2. Session reducer emits `aos.agent/ProfileLookupRequested@1` with selected `provider_profile_id`, `run_id`, and fence fields.
3. Profile catalog reducer handles lookup for `ProviderProfile@1`; catalog model metadata may validate compatibility and produce diagnostics only.
4. Profile catalog reducer emits either:
   - `aos.agent/ProfileLookupResolved@1` with concrete `provider`, `model`, and resolved optional controls, or
   - `aos.agent/ProfileLookupFailed@1` with typed failure reason.
5. On `ProfileLookupResolved@1`, session reducer snapshots immutable `active_run_config` (P2.1) containing `{provider_profile_id, provider, model, reasoning_effort?, max_tokens?}` and then emits/enters `RunStarted`.
6. On `ProfileLookupFailed@1`, session reducer transitions run to failure path with typed error; no `RunStarted` for that run.
7. For each LLM step, `LlmGenerateParams@1` is materialized from `active_run_config` plus deterministic step context (`message_refs`, tools, stop sequences, metadata, format refs).
8. No host/user step-level config override path exists in P2.2.
9. Capability checks still apply at effect boundary (`llm.basic` cap and policy).
10. Adapter receives concrete fully-resolved request values.
11. If required values remain unset after precedence resolution, run start fails with typed validation error.

Run-start resolution order:

1. Select run request config source:
   - `RunRequested.run_overrides` when present (full replacement), else `session_config`.
2. Resolve through `ProviderProfile@1` defaults/constraints via `ProfileCatalogReducer`.
3. No semantic defaults from host catalog/runtime.

Step materialization rule:

- Config-bearing fields (`provider`, `model`, `runtime.reasoning_effort`, `runtime.max_tokens`, and optional `provider_profile_id` audit tag) must come from immutable `active_run_config`.
- `correlation_id` should be materialized deterministically from run/turn/step context for tracing (or left unset).
- Step-local runtime fields may vary only from deterministic reducer/plan logic for that step.

Optional tuning controls rule:

- If `active_run_config.reasoning_effort` is set, materialize it into `runtime.reasoning_effort`; if unset, keep `runtime.reasoning_effort` unset so provider defaults apply.
- If `active_run_config.max_tokens` is set, materialize it into `runtime.max_tokens`; if unset, keep `runtime.max_tokens` unset so provider defaults apply.
- Cap enforcer and policy logic must treat missing `max_tokens` as valid input.

Only resolved values in `LlmGenerateParams@1` are authoritative for deterministic policy/cap and audit.

## Model Change Boundary

- Allowed: changing model/provider between runs in the same session.
- Not allowed: changing model/provider within an active run.

Any request to change model while a run is active is deferred to the next run (or rejected by host policy).

## Relationship to `tmp-llm-lib-code/02-coding-agent-loop-spec.md`

Adopt:

1. Provider-aligned philosophy over forced universal tool surface.
2. Runtime control over reasoning and provider options.

Do not adopt directly:

1. Untyped per-provider config blobs in core runtime state.
2. Provider-specific logic in reducer state machine.

Rule:

Provider-specific behavior belongs in profile and adapter layers; reducer-facing envelopes stay typed and provider-agnostic.

## Non-Goals

- Loop detection and context truncation policy (P2.3).
- Event stream API guarantees (P2.4).
- Failure taxonomy and retry ownership details (P2.5).

## Deliverables

1. Profile schemas and profile-catalog reducer helpers in `aos-agent-sdk`.
2. SDK AIR cleanup removing SDK ownership of `sys/Llm*` schema defs.
3. Updated core built-in schema set replacing `LlmGenerateParams@1` and `LlmGenerateReceipt@1`.
4. `aos-effects` + `aos-host` + cap enforcers/kernel updates aligned to the in-place core schema replacement.
5. SDK mapper helpers from `aos.agent/*` run/step state into core `sys/LlmGenerateParams@1`.
6. Manifest/cap/policy updates aligned to profile lookup request/result events.

## Exit Criteria

1. OpenAI and Anthropic profiles pass the same core contract tests for no-tool and tool-call flows.
2. Profile lookup via dedicated profile-catalog reducer (run-scoped full replacement when provided, otherwise session defaults, then profile defaults/constraints + catalog validation diagnostics) works without app-level provider branching.
3. `provider_options_ref` round-trip is verified from effect params to provider request.
4. `output_ref` normalized envelope and `raw_output_ref` provider payload are both populated where available.
5. `max_tokens` omitted path is covered and succeeds for supported models/providers.
6. Core `sys/*` ownership is preserved: no SDK AIR asset defines `sys/Llm*` schemas.
7. Replay parity preserved (no nondeterministic fields leak into reducer decisions).
8. Unknown `provider_profile_id` path produces deterministic `ProfileLookupFailed@1` and run failure with no partial run activation.
