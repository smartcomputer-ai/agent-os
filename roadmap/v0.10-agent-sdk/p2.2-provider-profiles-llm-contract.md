# P2.2: Provider Profiles and LLM Contract Evolution

**Stage**: P2.2  
**Status**: Proposed  
**Depends on**: P2.1

## Purpose

Standardize provider strategy and align LLM effect contracts with agent runtime controls.

This stage resolves concerns #2 (provider philosophy) and #7 (LLM contract pressure).

Because v0.10 accepts breaking changes, this stage replaces `sys/LlmGenerateParams@1` and `sys/LlmGenerateReceipt@1` in place instead of introducing parallel `@2` schemas.

## Why Provider Profiles Exist

`provider + model` is not enough for reliable behavior. Different providers differ on:

- tool-call protocol and shape,
- reasoning controls,
- streaming semantics,
- context-window characteristics,
- option passthrough format.

`ProviderProfile` is the contract that captures those differences once so app reducers/plans do not fork behavior per provider.

## Catalog-Backed, Thin Profiles

`crates/aos-llm/src/catalog_models.json` already carries model metadata (context window, capabilities, aliases).

P2.2 should not duplicate that whole catalog in AIR schemas.

Design split:

1. Catalog (`catalog_models.json`):
   - runtime metadata and compatibility hints.
2. AIR profile schema:
   - world-pinned selection and defaults/overrides needed for deterministic behavior and audit.

Key rule:

Anything that affects cap/policy decisions or effect intent semantics must be explicit in `LlmGenerateParams` (and thus journaled), not hidden behind mutable host catalog lookups.

## What This Stage Locks

1. `aos.agent/ProviderProfile@1` schema and semantics.
2. In-place replacement of `sys/LlmGenerateParams@1` and `sys/LlmGenerateReceipt@1`.
3. Normalized output envelope contract for `output_ref`.
4. Host adapter wiring from typed effect params into `aos-llm::Request`.

## Scope

1. Define provider profile contract:
   - `aos.agent/ProviderProfile@1`
   - thin world-pinned defaults (`provider`, `model`, default refs)
   - optional policy-level constraints (`allowed_models`, local max token cap)
   - catalog linkage via model id hints
2. Evolve LLM effect schemas:
   - replace `sys/LlmGenerateParams@1` in place
   - replace `sys/LlmGenerateReceipt@1` in place
3. Thread profile/runtime controls through host adapter:
   - nested runtime args (`runtime`)
   - reasoning/tool/stop/metadata/options controls
4. Keep normalized + raw output model:
   - `output_ref` for reducer-facing normalized envelope
   - `raw_output_ref` for provider-native audit/debug

## Why Nested `runtime` in `LlmGenerateParams@1`

With thin profiles, most per-call variation is runtime model args. Nesting keeps contracts clearer:

- top-level in `LlmGenerateParams@1`:
  - identity and determinism-critical routing/audit fields (`provider_profile_id`, `provider`, `model`, `message_refs`, `api_key`)
- nested `runtime`:
  - call-local knobs (`temperature`, optional `max_tokens`, `reasoning_effort`, tool/stop/options/format refs)

This avoids unbounded top-level growth and makes precedence resolution explicit (`runtime` overrides > profile defaults).

`provider` and `model` are intentionally not in `runtime` so they cannot drift mid-run as per P2.1 run-immutability rules.

## Proposed Schema Shapes (AIR-style)

### Provider Profile Identity

`aos.agent/ProviderProfileId@1` and `aos.agent/ReasoningEffort@1` are defined in P2.1 and reused here.

```json
{
  "$kind": "defschema",
  "name": "aos.agent/ProviderProfile@1",
  "type": {
    "record": {
      "profile_id": { "ref": "aos.agent/ProviderProfileId@1" },
      "provider": { "text": {} },
      "default_model": { "text": {} },
      "catalog_model_id": { "option": { "text": {} } },
      "allowed_models": { "option": { "set": { "text": {} } } },
      "max_tokens_cap": { "option": { "nat": {} } },
      "default_reasoning_effort": {
        "option": { "ref": "aos.agent/ReasoningEffort@1" }
      },
      "provider_options_default_ref": { "option": { "hash": {} } },
      "response_format_default_ref": { "option": { "hash": {} } }
    }
  }
}
```

This profile is intentionally thin.

- Provider capability flags (streaming/tool parallel/etc.) come from catalog/runtime resolver logic, not duplicated in AIR.
- `catalog_model_id` is optional linkage for host-side validation and UI hints.
- `provider`, `default_model`, and caps-relevant constraints remain explicit in AIR.

### LLM Runtime Args (new helper schema)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmRuntimeArgs@1",
  "type": {
    "record": {
      "temperature": { "option": { "dec128": {} } },
      "max_tokens": { "option": { "nat": {} } },
      "tool_refs": { "option": { "list": { "hash": {} } } },
      "tool_choice": { "option": { "ref": "sys/LlmToolChoice@1" } },
      "reasoning_effort": { "option": { "ref": "aos.agent/ReasoningEffort@1" } },
      "stop_sequences": { "option": { "list": { "text": {} } } },
      "metadata": {
        "option": {
          "map": {
            "key": { "text": {} },
            "value": { "text": {} }
          }
        }
      },
      "provider_options_ref": { "option": { "hash": {} } },
      "response_format_ref": { "option": { "hash": {} } }
    }
  }
}
```

### LLM Params (replaced `@1`, now with nested runtime args)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateParams@1",
  "type": {
    "record": {
      "provider_profile_id": { "ref": "aos.agent/ProviderProfileId@1" },
      "provider": { "text": {} },
      "model": { "text": {} },
      "message_refs": { "list": { "hash": {} } },
      "runtime": { "ref": "sys/LlmRuntimeArgs@1" },
      "api_key": { "option": { "ref": "sys/TextOrSecretRef@1" } }
    }
  }
}
```

Notes:

- Keep `provider` and `model` explicit in params for cap enforcement, policy evaluation, and audit.
- `provider_profile_id` ties the request to a profile-defined default/constraints bundle.
- `runtime` groups per-call model args cleanly and avoids endless top-level field growth.
- `provider` and `model` are resolved from run config and are fixed for the entire run.
- `runtime.max_tokens` is optional; when absent, adapter passes no max-token override and provider defaults apply.

### LLM Receipt (replaced `@1`)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateReceipt@1",
  "type": {
    "record": {
      "output_ref": { "hash": {} },
      "raw_output_ref": { "option": { "hash": {} } },
      "provider_response_id": { "option": { "text": {} } },
      "finish_reason": {
        "record": {
          "reason": { "text": {} },
          "raw": { "option": { "text": {} } }
        }
      },
      "token_usage": {
        "record": {
          "prompt": { "nat": {} },
          "completion": { "nat": {} }
        }
      },
      "usage_details": {
        "option": {
          "record": {
            "reasoning_tokens": { "option": { "nat": {} } },
            "cache_read_tokens": { "option": { "nat": {} } },
            "cache_write_tokens": { "option": { "nat": {} } }
          }
        }
      },
      "warnings_ref": { "option": { "hash": {} } },
      "cost_cents": { "option": { "nat": {} } },
      "provider_id": { "text": {} }
    }
  }
}
```

## Normalized Output Envelope (`output_ref`)

`output_ref` must point to a provider-agnostic JSON envelope in CAS, sufficient for reducers to parse tool calls without provider-specific branching.

Proposed schema:

```json
{
  "$kind": "defschema",
  "name": "aos.agent/LlmOutputEnvelope@1",
  "type": {
    "record": {
      "assistant_text": { "option": { "text": {} } },
      "tool_calls_ref": { "option": { "hash": {} } },
      "reasoning_ref": { "option": { "hash": {} } }
    }
  }
}
```

Tool-call entries (stored at `tool_calls_ref`) should carry:
- stable call id,
- canonical tool name,
- normalized arguments JSON string or canonical arguments object.

`raw_output_ref` preserves full provider-native output for debug/audit.

## Mapping to `aos-llm` and OpenAI Responses API

Current status in codebase:

- `aos-llm::Request` already supports:
  - `reasoning_effort`,
  - `provider_options`,
  - `response_format`,
  - `metadata`,
  - `stop_sequences`.
- `aos-host` LLM adapter currently populates:
  - `output_ref`,
  - `raw_output_ref`,
  - token usage,
  but drops some advanced fields from effect params.

Expected mapping example (OpenAI Responses):

1. `sys/LlmGenerateParams@1.model` -> `request.model`
2. `runtime.reasoning_effort` -> `request.reasoning_effort`
3. `runtime.provider_options_ref` JSON -> `request.provider_options.openai.*`
4. provider returns `response.id` -> `provider_response_id`
5. normalized envelope -> `output_ref`
6. raw provider JSON -> `raw_output_ref`

Equivalent flow for Anthropic maps to `provider_options.anthropic.*` and Messages-specific semantics under same receipt contract.

## Profile Resolution Semantics

1. Reducer or plan chooses profile id.
2. Profile resolver loads `ProviderProfile@1`.
3. At run start, resolver computes concrete `provider` and `model` for the run.
4. Reducer snapshots this as active run config (P2.1).
5. For each LLM step in the run, resolver merges defaults and per-call overrides into `runtime` only.
6. Capability checks still apply at effect boundary (`llm.basic` cap and policy).
7. Adapter receives concrete fully-resolved request values.

Precedence order:

1. Explicit per-call runtime fields.
2. Profile defaults.
3. Host/catalog inferred defaults (non-critical hints only).

`max_tokens` rule:

- If `runtime.max_tokens` is set, pass it through as request override.
- If `runtime.max_tokens` is unset, leave unset and use provider/model default behavior.
- Cap enforcer and policy logic must treat missing `max_tokens` as valid input.

Only resolved values in `LlmGenerateParams@1` are authoritative for deterministic policy/cap and audit.

## Model Change Boundary

- Allowed: changing model/provider between runs in the same session.
- Not allowed: changing model/provider within an active run.

Any request to change model while a run is active is deferred to the next run (or rejected by host policy).

## Relationship to `tmp-llm-lib-code/02-coding-agent-loop-spec.md`

Adopt:

1. Provider-aligned philosophy over forced universal tool surface.
2. Runtime control over reasoning and provider options.

Do not adopt directly:

1. Untyped per-provider config blobs in core runtime state.
2. Provider-specific logic in reducer state machine.

Rule:

Provider-specific behavior belongs in profile and adapter layers; reducer-facing envelopes stay typed and provider-agnostic.

## Non-Goals

- Loop detection and context truncation policy (P2.3).
- Event stream API guarantees (P2.4).
- Failure taxonomy and retry ownership details (P2.5).

## Deliverables

1. Profile schemas and resolver helpers in `aos-agent-sdk`.
2. Updated built-in schema set replacing `LlmGenerateParams@1` and `LlmGenerateReceipt@1`.
3. `aos-host` LLM adapter updates to consume/pass expanded fields into `aos-llm`.
4. Manifest/cap/policy updates aligned to the in-place schema replacement.

## Exit Criteria

1. OpenAI and Anthropic profiles pass the same core contract tests for no-tool and tool-call flows.
2. Profile resolution (profile defaults + runtime overrides + catalog hints) works without app-level provider branching.
3. `provider_options_ref` round-trip is verified from effect params to provider request.
4. `output_ref` normalized envelope and `raw_output_ref` provider payload are both populated where available.
5. `max_tokens` omitted path is covered and succeeds for supported models/providers.
6. Replay parity preserved (no nondeterministic fields leak into reducer decisions).

## Normative Contract Update (Determinism and Tool Call Canonicalization)

The sections below are normative and override earlier ambiguities in this document.

### Typed `provider_profile_id` in LLM params

`sys/LlmGenerateParams@1.provider_profile_id` must reference
`aos.agent/ProviderProfileId@1`.

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateParams@1",
  "type": {
    "record": {
      "provider_profile_id": { "ref": "aos.agent/ProviderProfileId@1" },
      "provider": { "text": {} },
      "model": { "text": {} },
      "message_refs": { "list": { "hash": {} } },
      "runtime": { "ref": "sys/LlmRuntimeArgs@1" },
      "api_key": { "option": { "ref": "sys/TextOrSecretRef@1" } }
    }
  }
}
```

### Resolver determinism boundary

To preserve replay parity, resolver behavior is constrained:

1. Catalog lookup may validate compatibility and produce diagnostics only.
2. Catalog lookup may not inject or mutate cap/policy-relevant values.
3. Resolved `provider`, `model`, `runtime`, and profile id in effect params are
   the only authoritative values for cap/policy decisions.
4. If required values are absent after applying profile defaults and per-call
   overrides, resolver must fail with typed validation error rather than infer
   host-side defaults.

Updated precedence:

1. Explicit per-call runtime fields.
2. Profile defaults.
3. No semantic defaults from host catalog/runtime.

### Canonical normalized tool-call schema

`tool_calls_ref` must point to a typed list of canonical tool calls. Mixed
representations ("JSON string or object") are not allowed.

```json
{
  "$kind": "defschema",
  "name": "aos.agent/LlmToolCall@1",
  "type": {
    "record": {
      "call_id": { "text": {} },
      "tool_name": { "text": {} },
      "arguments_ref": { "hash": {} },
      "provider_call_id": { "option": { "text": {} } }
    }
  }
}
```

```json
{
  "$kind": "defschema",
  "name": "aos.agent/LlmToolCallList@1",
  "type": { "list": { "ref": "aos.agent/LlmToolCall@1" } }
}
```

`arguments_ref` must reference canonical JSON object payload in CAS. Reducers must
never parse provider-specific argument strings.

Updated envelope:

```json
{
  "$kind": "defschema",
  "name": "aos.agent/LlmOutputEnvelope@1",
  "type": {
    "record": {
      "assistant_text": { "option": { "text": {} } },
      "tool_calls_ref": { "option": { "hash": {} } },
      "reasoning_ref": { "option": { "hash": {} } }
    }
  }
}
```

When `tool_calls_ref` is present, the referenced CAS payload must validate as
`aos.agent/LlmToolCallList@1`.

### Parallel tool intent normalization

When a model returns multiple tool calls:

1. Adapter must preserve model-emitted call ids as `call_id`.
2. Adapter must canonicalize call ordering by emitted sequence and include that
   sequence in serialized list order.
3. Reducer must use `call_id` identity for idempotency and stale-result handling.
