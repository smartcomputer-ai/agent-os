# P2.2: Direct Provider/Model LLM Contract Evolution

**Stage**: P2.2  
**Status**: In Progress (mostly complete)  
**Depends on**: P2.1

## Implementation Progress (2026-02-13)

Completed:

1. Removed provider-profile/profile-lookup scaffolding from `aos-agent-sdk` contracts, AIR assets, and reducer bins.
2. Rebased SDK contracts to direct provider/model run config:
   - `SessionConfig@1` and `RunConfig@1` now carry `provider` + `model`.
3. Core `sys/Llm*` contract shapes are present in primary ownership locations:
   - `spec/defs/builtin-schemas.air.json`,
   - `crates/aos-effects/src/builtins/mod.rs`,
   - `crates/aos-host/src/adapters/llm.rs`.
4. Build status:
   - `cargo check -p aos-agent-sdk` passes.

Remaining:

1. Add SDK deterministic mapper helpers for:
   - `RunConfig + StepContext -> sys/LlmGenerateParams@1`.
2. Add/confirm deterministic integration coverage focused on SDK mapping + replay parity under P2.2 acceptance criteria.
3. Close any remaining P2.2-specific conformance assertions across cap enforcer/runtime tests.

## Ownership Boundary (Authoritative)

1. Core owns `sys/*`:
   - built-in schemas/effects/caps under `spec/defs/*`,
   - low-level effect/receipt Rust types under `aos-effects`,
   - adapter execution under `aos-host`.
2. SDK owns `aos.agent/*`:
   - session/run/turn/step contracts,
   - reducer-facing typed envelopes/events,
   - deterministic mapper helpers producing core effect params.
3. Rule:
   - SDK AIR assets must not define or replace `sys/*` schemas.
   - `sys/*` schemas must not depend on `aos.agent/*` references.

## Purpose

Align the LLM effect contract with agent runtime controls while keeping configuration simple:

- run config explicitly carries `provider` and `model`,
- reducers/plans materialize `llm.generate` directly from immutable run config,
- no profile registry or lookup choreography.

Because v0.10 accepts breaking changes, this stage replaces `sys/LlmGenerateParams@1` and `sys/LlmGenerateReceipt@1` in place.

## What This Stage Locks

1. Core-owned in-place replacement/evolution of `sys/LlmGenerateParams@1` and `sys/LlmGenerateReceipt@1`.
2. Normalized output envelope contract for `output_ref`.
3. Host adapter wiring from typed effect params into `aos-llm::Request`.
4. Deterministic run-to-step mapping rules from `aos.agent/RunConfig@1` to `sys/LlmGenerateParams@1`.

## Scope

1. Evolve LLM effect schemas in core (not SDK):
   - replace `sys/LlmGenerateParams@1` in place,
   - replace `sys/LlmGenerateReceipt@1` in place.
2. Thread runtime controls through host adapter:
   - nested runtime args (`runtime`),
   - reasoning/tool/stop/metadata/options controls.
3. Keep normalized + raw output model:
   - `output_ref` for reducer-facing normalized envelope,
   - `raw_output_ref` for provider-native audit/debug.
4. Keep provider/model selection explicit in session/run config with immutable per-run snapshot.

## Why Nested `runtime` in `LlmGenerateParams@1`

- top-level fields remain identity/routing/audit oriented (`correlation_id`, `provider`, `model`, `message_refs`, `api_key`).
- `runtime` groups execution tuning (`max_tokens`, `reasoning_effort`, tools, stop, metadata, provider options) to avoid uncontrolled top-level growth.

`provider` and `model` are intentionally top-level and must not drift mid-run.

## Proposed Schema Shapes (AIR-style)

### LLM Runtime Args (core helper schema)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmRuntimeArgs@1",
  "type": {
    "record": {
      "temperature": { "option": { "dec128": {} } },
      "max_tokens": { "option": { "nat": {} } },
      "tool_refs": { "option": { "list": { "hash": {} } } },
      "tool_choice": { "option": { "ref": "sys/LlmToolChoice@1" } },
      "reasoning_effort": { "option": { "text": {} } },
      "stop_sequences": { "option": { "list": { "text": {} } } },
      "metadata": {
        "option": {
          "map": {
            "key": { "text": {} },
            "value": { "text": {} }
          }
        }
      },
      "provider_options_ref": { "option": { "hash": {} } },
      "response_format_ref": { "option": { "hash": {} } }
    }
  }
}
```

### LLM Params (replaced `@1`)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateParams@1",
  "type": {
    "record": {
      "correlation_id": { "option": { "text": {} } },
      "provider": { "text": {} },
      "model": { "text": {} },
      "message_refs": { "list": { "hash": {} } },
      "runtime": { "ref": "sys/LlmRuntimeArgs@1" },
      "api_key": { "option": { "ref": "sys/TextOrSecretRef@1" } }
    }
  }
}
```

Notes:

- `provider` and `model` stay explicit for cap enforcement, policy evaluation, and audit.
- `correlation_id` is optional and agent-agnostic.
- `reasoning_effort` in `sys/*` remains agent-agnostic text (SDK maps its enum before effect emission).
- Optional controls (`runtime.reasoning_effort`, `runtime.max_tokens`) are sourced from immutable run config when set; otherwise they remain unset and provider defaults apply.

### LLM Receipt (replaced `@1`)

```json
{
  "$kind": "defschema",
  "name": "sys/LlmGenerateReceipt@1",
  "type": {
    "record": {
      "output_ref": { "hash": {} },
      "raw_output_ref": { "option": { "hash": {} } },
      "provider_response_id": { "option": { "text": {} } },
      "finish_reason": {
        "record": {
          "reason": { "text": {} },
          "raw": { "option": { "text": {} } }
        }
      },
      "token_usage": {
        "record": {
          "prompt": { "nat": {} },
          "completion": { "nat": {} }
        }
      },
      "usage_details": {
        "option": {
          "record": {
            "reasoning_tokens": { "option": { "nat": {} } },
            "cache_read_tokens": { "option": { "nat": {} } },
            "cache_write_tokens": { "option": { "nat": {} } }
          }
        }
      },
      "warnings_ref": { "option": { "hash": {} } },
      "cost_cents": { "option": { "nat": {} } },
      "provider_id": { "text": {} }
    }
  }
}
```

## Normalized Output Envelope (`output_ref`)

`output_ref` must point to a provider-agnostic typed envelope in CAS so reducers do not branch on provider-specific wire shape.

Canonical tool-call schema:

```json
{
  "$kind": "defschema",
  "name": "sys/LlmToolCall@1",
  "type": {
    "record": {
      "call_id": { "text": {} },
      "tool_name": { "text": {} },
      "arguments_ref": { "hash": {} },
      "provider_call_id": { "option": { "text": {} } }
    }
  }
}
```

```json
{
  "$kind": "defschema",
  "name": "sys/LlmToolCallList@1",
  "type": { "list": { "ref": "sys/LlmToolCall@1" } }
}
```

Envelope schema:

```json
{
  "$kind": "defschema",
  "name": "sys/LlmOutputEnvelope@1",
  "type": {
    "record": {
      "assistant_text": { "option": { "text": {} } },
      "tool_calls_ref": { "option": { "hash": {} } },
      "reasoning_ref": { "option": { "hash": {} } }
    }
  }
}
```

## Run-Start and Step Materialization Semantics

1. At run start, choose config source:
   - `RunRequested.run_overrides` when present (full replacement), otherwise
   - `session_config` defaults.
2. Validate required fields (`provider`, `model`) are present.
3. Snapshot immutable `active_run_config`.
4. Enter `RunStarted`.
5. For each LLM step, materialize `sys/LlmGenerateParams@1` from:
   - `active_run_config` (`provider`, `model`, optional `reasoning_effort`, optional `max_tokens`),
   - deterministic step context (`message_refs`, tools, stop sequences, metadata, response format/options refs).
6. No host/user step-level override path in P2.2.

Determinism rule:

- Any value that affects capability/policy decisions or effect semantics must be explicit in `LlmGenerateParams@1` and journaled.
- Host catalog lookups may validate and produce diagnostics, but may not mutate params.

## Model Change Boundary

- Allowed: changing provider/model between runs in the same session.
- Not allowed: changing provider/model within an active run.

Any in-run change request is deferred to the next run (or rejected by policy).

## Non-Goals

- Provider profile registry/lookup architecture.
- Loop detection and context truncation policy (P2.3).
- Event stream API guarantees (P2.4).
- Failure taxonomy and retry ownership details (P2.5).

## Deliverables

1. SDK cleanup removing profile schemas/reducers/events.
2. Updated core built-in schema set replacing `LlmGenerateParams@1` and `LlmGenerateReceipt@1`.
3. `aos-effects` + `aos-host` + cap enforcer updates aligned to core schema replacement.
4. SDK mapper helpers from run/step state into core `sys/LlmGenerateParams@1`.
5. Manifest/cap/policy updates aligned to direct provider/model run config.

## Exit Criteria

1. OpenAI and Anthropic paths pass the same core contract tests for no-tool and tool-call flows.
2. `provider_options_ref` round-trip is verified from effect params to provider request.
3. `output_ref` normalized envelope and `raw_output_ref` provider payload are both populated where available.
4. `max_tokens` omitted path is covered and succeeds for supported models/providers.
5. Core `sys/*` ownership is preserved: no SDK AIR asset defines `sys/Llm*` schemas.
6. Replay parity preserved (no nondeterministic fields leak into reducer decisions).
7. Unknown provider/model validation path deterministically fails run start with no partial run activation.
